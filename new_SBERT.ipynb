{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lizbethlulu/Research-/blob/main/new_SBERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "gNqQ5EEMY8U_",
        "outputId": "49b6762e-1eb0-4b40-db75-49cf9018ab74"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting sentence-transformers\n",
            "  Downloading sentence_transformers-2.7.0-py3-none-any.whl (171 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m171.5/171.5 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: transformers<5.0.0,>=4.34.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.40.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.4)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.2.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.25.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.11.4)\n",
            "Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.20.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (9.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (3.14.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2.31.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (4.11.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (24.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (2023.12.25)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.4.3)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, sentence-transformers\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 sentence-transformers-2.7.0\n"
          ]
        }
      ],
      "source": [
        "!pip install sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mSOUgwGRZBjC"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "# from sentence_transformers import SentenceTransformer\n",
        "# model = SentenceTransformer('distiluse-base-multilingual-cased')\n",
        "import numpy as np\n",
        "\n",
        "from sentence_transformers import SentenceTransformer, models\n",
        "\n",
        "# Define the model name for Indonesian sentiment analysis\n",
        "model_name = 'ayameRushia/bert-base-indonesian-1.5G-sentiment-analysis-smsa'\n",
        "\n",
        "# Configure the SentenceTransformer model\n",
        "word_embedding_model = models.Transformer(model_name)\n",
        "pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(),\n",
        "                               pooling_mode_mean_tokens=True,\n",
        "                               pooling_mode_cls_token=False,\n",
        "                               pooling_mode_max_tokens=False)\n",
        "model = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PZ93ggf8ZmlA"
      },
      "outputs": [],
      "source": [
        "# df = pd.read_csv(\"https://drive.google.com/uc?export=download&id=1y8MlToeRw6avPU7qN47p3zIhhQuWKqMs\")\n",
        "df = pd.read_csv(\"https://drive.google.com/uc?export=download&id=1Xv14IoQjRKuYhji5Zadj4TD1SN8vkIdJ\")\n",
        "\n",
        "\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9VI01nwMabAh"
      },
      "outputs": [],
      "source": [
        "dicts = df.to_dict(orient='list')\n",
        "dicts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UwAHQnJFafh3"
      },
      "outputs": [],
      "source": [
        "# import os\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nPdMypJnab0r"
      },
      "outputs": [],
      "source": [
        "# def import_data(csv_file_path):\n",
        "#     datas = {}\n",
        "#     if os.path.exists(csv_file_path):\n",
        "#         # Read data from CSV\n",
        "#         df = pd.read_csv(csv_file_path)\n",
        "#         datas = df.to_dict(orient='list')\n",
        "#         print(\"Columns in the CSV file:\", list(datas.keys()))\n",
        "#     else:\n",
        "#         print(\"File does not exist. Creating a new dictionary.\")\n",
        "#     return datas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UOpiOwevahcJ"
      },
      "outputs": [],
      "source": [
        "# csv_file_path = \"/content/Fix_data.csv\"\n",
        "# print(csv_file_path)\n",
        "# dicts = import_data(\"Fix_data.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "04DOmL24atFf"
      },
      "outputs": [],
      "source": [
        "len(dicts.keys())\n",
        "dicts.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_gkYaRYDble-"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "# datas = {}\n",
        "# for key in tqdm(dicts.keys(),\"Sentence Embedding\"):\n",
        "#     tmp = dicts[key]['label'].copy()\n",
        "#     tmp.extend(dicts[key]['comments'])\n",
        "#     # tmp = df[df[\"label\"] == key][\"comments\"].tolist()\n",
        "#     # tmp.extend(df[df[\"label\"] == key][\"comments\"].tolist())\n",
        "#     if len(tmp) == 0:\n",
        "#         print(f\"label {key} has no posts or tweets\")\n",
        "#         continue\n",
        "#     arr = model.encode(tmp[0]).reshape(1, -1)\n",
        "#     for t in tmp[1:]:\n",
        "#         brr = model.encode(str(t))\n",
        "#         arr = np.concatenate((arr, brr.reshape(1, -1)), axis=0)\n",
        "#         print(f\"Finished encoding tweet from label {key}\")\n",
        "#     datas[key] = np.mean(arr, axis=0)\n",
        "\n",
        "# datas = {}\n",
        "# for i in tqdm(range(len(dicts['label'])), \"Sentence Embedding\"):\n",
        "#     label = dicts['label'][i]\n",
        "#     comment = dicts['comments'][i]\n",
        "#     if not comment:\n",
        "#         print(f\"Label {label} has no posts or tweets\")\n",
        "#         continue\n",
        "#     arr = model.encode(comment).reshape(1, -1)\n",
        "#     if label in datas:\n",
        "#         datas[label] = np.concatenate((datas[label], arr), axis=0)\n",
        "#     else:\n",
        "#         datas[label] = arr\n",
        "#     print(f\"Finished encoding tweet for label {label}\")\n",
        "\n",
        "# for label in datas:\n",
        "#     datas[label] = np.mean(datas[label], axis=0)\n",
        "\n",
        "# print(datas)\n",
        "\n",
        "datas = {}\n",
        "for i in tqdm(range(len(dicts['label'])), \"Sentence Embedding\"):\n",
        "    label = dicts['label'][i]\n",
        "    comment = dicts['comments'][i]\n",
        "    if not comment:\n",
        "        print(f\"Label {label} has no posts or tweets\")\n",
        "        continue\n",
        "    arr = model.encode(comment).reshape(1, -1)\n",
        "    if label in datas:\n",
        "        datas[label].append(arr)\n",
        "    else:\n",
        "        datas[label] = [arr]\n",
        "    print(f\"Finished encoding tweet for label {label}\")\n",
        "\n",
        "# print(datas)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Wr6b-QEhACkt"
      },
      "outputs": [],
      "source": [
        "datas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EnAnn3_Ybqun"
      },
      "outputs": [],
      "source": [
        "# encoded_datas = datas[list(datas.keys())[0]].reshape(1, -1)\n",
        "# for label in list(datas.keys())[1:]:\n",
        "#     encoded_datas = np.concatenate((encoded_datas, datas[label].reshape(1, -1)), axis=0)\n",
        "\n",
        "for label in datas:\n",
        "    datas[label] = np.array(datas[label])\n",
        "\n",
        "# Menggabungkan semua numpy array menjadi satu array besar\n",
        "encoded_datas = np.concatenate(list(datas.values()), axis=0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rImIAW-bbrjo"
      },
      "outputs": [],
      "source": [
        "print(len(list(datas.keys())),encoded_datas.shape)\n",
        "encoded_datas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CvJrnmztbuGD"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import DBSCAN\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import silhouette_score as ss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5MMZ8QRgxBLh"
      },
      "outputs": [],
      "source": [
        "pca = PCA(n_components=2)\n",
        "result = pca.fit_transform(encoded_datas)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DNcj8h8axJ8b"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "from sklearn.datasets import make_blobs\n",
        "import matplotlib.pyplot as plt\n",
        "sns.scatterplot(x=[X[0] for X in result],\n",
        "                y=[X[1] for X in result],\n",
        "                palette=\"deep\",\n",
        "                legend=None\n",
        "                )\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"y\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yNmLhmVDv_OB"
      },
      "outputs": [],
      "source": [
        "clustering = DBSCAN(eps=0.2, min_samples=2).fit(encoded_datas)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WoQQWge3wLhT"
      },
      "outputs": [],
      "source": [
        "labels = clustering.labels_\n",
        "N_clus=len(set(labels))-(1 if -1 in labels else 0)\n",
        "print('Estimated no. of clusters: %d' % N_clus)\n",
        "\n",
        "# Identify Noise\n",
        "n_noise = list(clustering.labels_).count(-1)\n",
        "print('Estimated no. of noise points: %d' % n_noise)\n",
        "ss(encoded_datas, labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bvJs44ne3wsz"
      },
      "outputs": [],
      "source": [
        "epsilon = np.linspace(0.1,1,num=5)\n",
        "epsilon\n",
        "min_samples = np.arange(2,10,step=3)\n",
        "min_samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GBdzGPRe39Ef"
      },
      "outputs": [],
      "source": [
        "import itertools\n",
        "combinations = list(itertools.product(epsilon,min_samples))\n",
        "combinations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J40gMIit4GlR"
      },
      "outputs": [],
      "source": [
        "all_parameters = []\n",
        "\n",
        "def get_scores_and_labels(combinations, encoded_datas):\n",
        "    scores = []\n",
        "    all_labels_list = []\n",
        "    n = len(combinations)  # Added to get the number of combinations for print statements\n",
        "    for i, (eps, num_samples) in enumerate(combinations):\n",
        "        clustering = DBSCAN(eps=eps, min_samples=num_samples).fit(encoded_datas)\n",
        "        labels = clustering.labels_\n",
        "        labels_set = set(labels)\n",
        "        num_clusters = len(labels_set)\n",
        "        if -1 in labels_set:\n",
        "            num_clusters -= 1\n",
        "        if (num_clusters < 2) or (num_clusters > 50):\n",
        "            scores.append(-10)\n",
        "            all_labels_list.append(\"bad\")\n",
        "            c = (eps, num_samples)\n",
        "            print(f\"combination {c} on iteration {i+1} of {n} has {num_clusters} clusters. next!\")\n",
        "            continue\n",
        "        ss_val = ss(encoded_datas, labels)\n",
        "        outliers = np.sum(labels == -1)\n",
        "        scores.append(ss_val)\n",
        "        all_labels_list.append(labels)\n",
        "        parameters = {\n",
        "            'epsilon': eps,\n",
        "            'min_samples': num_samples,\n",
        "            'score': ss_val,\n",
        "            'num_clusters': num_clusters,\n",
        "            'outliers': outliers\n",
        "        }\n",
        "        all_parameters.append(parameters)\n",
        "        print(f\"index {i}, score: {scores[-1]}, labels: {all_labels_list[-1]}, numClusters: {num_clusters}\")\n",
        "        parameters\n",
        "    best_index = np.argmax(scores)\n",
        "    best_parameters = combinations[best_index]\n",
        "    best_labels = all_labels_list[best_index]\n",
        "    best_scores = scores[best_index]\n",
        "    return {\n",
        "        'best_epsilon': best_parameters[0],\n",
        "        'best_min_samples': best_parameters[1],\n",
        "        'best_labels': best_labels,\n",
        "        'best_score': best_scores\n",
        "    }\n",
        "\n",
        "best_dic = get_scores_and_labels(combinations,encoded_datas)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ZUIdvLS4h9r"
      },
      "outputs": [],
      "source": [
        "pr = pd.DataFrame(all_parameters)\n",
        "pr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uA7uFZAC6kbY"
      },
      "outputs": [],
      "source": [
        "# encoded_datas[0]\n",
        "best_dic\n",
        "# df['cluster'] = best"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5PmHVwK_6lEO"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "cosine_similarity(encoded_datas[48].reshape(1,-1),encoded_datas[50].reshape(1,-1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CgsTczbY6n-f"
      },
      "outputs": [],
      "source": [
        "df['cluster'] = best_dic['best_labels']\n",
        "df\n",
        "df['cluster'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "20ZvCfoR6pmu"
      },
      "outputs": [],
      "source": [
        "labels = df['cluster']\n",
        "# labels\n",
        "clusters = {}\n",
        "for label in set(labels):\n",
        "    clusters[label] = df[df['cluster'] == label]['comments'].tolist()\n",
        "    # print(f\"labels : {label}, clusters :  {len(clusters[label])}\")\n",
        "\n",
        "clusters[-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-uA2OfD36rOy"
      },
      "outputs": [],
      "source": [
        "from gensim.models import LdaModel\n",
        "from gensim.corpora import Dictionary\n",
        "\n",
        "def apply_lda_to_clusters(clusters, num_topics):\n",
        "    lda_models = {}\n",
        "    for label, documents in clusters.items():\n",
        "        print(f\"Processing cluster {label} with {len(documents)} documents\")\n",
        "        if len(documents) == 0:\n",
        "            print(f\"Skipping cluster {label} because it is empty\")\n",
        "            continue\n",
        "        tokenized_docs = [doc.split() for doc in documents]\n",
        "        dictionary = Dictionary(tokenized_docs)\n",
        "        if len(dictionary) == 0:\n",
        "            print(f\"Skipping cluster {label} because dictionary is empty\")\n",
        "            continue\n",
        "        corpus = [dictionary.doc2bow(doc) for doc in tokenized_docs]\n",
        "        if len(corpus) == 0:\n",
        "            print(f\"Skipping cluster {label} because corpus is empty\")\n",
        "            continue  # Skip clusters with empty corpus\n",
        "        # Melatih model LDA\n",
        "        lda_model = LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=10)\n",
        "        lda_models[label] = {\n",
        "            'lda_model': lda_model,\n",
        "            'corpus': corpus,\n",
        "            'dictionary': dictionary\n",
        "        }\n",
        "        print(f\"Finished processing cluster {label}\")\n",
        "    return lda_models\n",
        "\n",
        "# Terapkan LDA pada setiap kelompok dokumen\n",
        "num_topics = 1\n",
        "lda_models = apply_lda_to_clusters(clusters, num_topics=num_topics)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wp5Ixa4y6sBP"
      },
      "outputs": [],
      "source": [
        "!pip install wordcloud"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4kNdYAqj6vJn"
      },
      "outputs": [],
      "source": [
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def display_topics_and_wordcloud(lda_model, num_topics, num_words=10):\n",
        "    for topic_num in range(num_topics):\n",
        "        topic_words = lda_model.show_topic(topic_num, topn=num_words)\n",
        "        topic_words_str = \", \".join([f\"{word} ({prob:.9f})\" for word, prob in topic_words])\n",
        "        print(f\"Topik {topic_num}: {topic_words_str}\")\n",
        "        word_freq = {word: prob for word, prob in topic_words}\n",
        "        wordcloud = WordCloud(width=800, height=400, background_color='black').generate_from_frequencies(word_freq)\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        plt.imshow(wordcloud, interpolation='bilinear')\n",
        "        plt.axis('off')\n",
        "        plt.title(f'Topic {topic_num + 1}')\n",
        "        plt.show()\n",
        "\n",
        "# Penggunaan:\n",
        "for label, model_info in lda_models.items():\n",
        "    print(f\"Topik untuk kluster {label}:\")\n",
        "    display_topics_and_wordcloud(model_info['lda_model'], num_topics=num_topics, num_words=900)\n",
        "    print(\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YZAZ3znw6yUu"
      },
      "outputs": [],
      "source": [
        "cluster_distribution = df.groupby(['sosmed', 'cluster']).size().unstack(fill_value=0)\n",
        "print(cluster_distribution)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}